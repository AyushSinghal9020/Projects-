# -*- coding: utf-8 -*-
"""health-insurance-prediction-eda.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Yr-MkhkmpUOxmtqm5_Wvb1biDRMV-FSz

# 1 | Health Insurance Prediction ‚ù§Ô∏è‚Äçü©π
<img src = "https://pbs.twimg.com/media/EDORK3cUEAAr4As.jpg">

## About Dataset

## Context
Our client is an Insurance company that has provided Health Insurance to its customers now they need your help in building a model to predict whether the policyholders (customers) from past year will also be interested in Vehicle Insurance provided by the company.

An insurance policy is an arrangement by which a company undertakes to provide a guarantee of compensation for specified loss, damage, illness, or death in return for the payment of a specified premium. A premium is a sum of money that the customer needs to pay regularly to an insurance company for this guarantee.

For example, you may pay a premium of Rs. 5000 each year for a health insurance cover of Rs. 200,000/- so that if, God forbid, you fall ill and need to be hospitalised in that year, the insurance provider company will bear the cost of hospitalisation etc. for upto Rs. 200,000. Now if you are wondering how can company bear such high hospitalisation cost when it charges a premium of only Rs. 5000/-, that is where the concept of probabilities comes in picture. For example, like you, there may be 100 customers who would be paying a premium of Rs. 5000 every year, but only a few of them (say 2-3) would get hospitalised that year and not everyone. This way everyone shares the risk of everyone else.

Just like medical insurance, there is vehicle insurance where every year customer needs to pay a premium of certain amount to insurance provider company so that in case of unfortunate accident by the vehicle, the insurance provider company will provide a compensation (called ‚Äòsum assured‚Äô) to the customer.

Building a model to predict whether a customer would be interested in Vehicle Insurance is extremely helpful for the company because it can then accordingly plan its communication strategy to reach out to those customers and optimise its business model and revenue.

Now, in order to predict, whether the customer would be interested in Vehicle insurance, you have information about demographics (gender, age, region code type), Vehicles (Vehicle Age, Damage), Policy (Premium, sourcing channel) etc.

## Data Description
|Variable|Definition|
|---|---|
|id|Unique ID for the customer
|Gender|Gender of the customer
|Age|Age of the customer
|Driving_License|0 : Customer does not have DL
||1 : Customer already has DL
|Region_Code|Unique code for the region of the customer
|Previously_Insured|1 : Customer already has Vehicle Insurance
||0 : Customer doesn't have Vehicle Insurance
|Vehicle_Age|Age of the Vehicle
|Vehicle_Damage|1 : Customer got his/her vehicle damaged in the past
||0 : Customer didn't get his/her vehicle damaged in the past.
|Annual_Premium|The amount customer needs to pay as premium in the year
|Policy_Sales_Channel|Anonymized Code for the channel of outreaching to the customer
|Vintage|Number of Days, Customer has been associated with the company
|Response|1 : Customer is interested
||0 : Customer is not interested

## Evaluation Metric
The evaluation metric for this hackathon is ROC_AUC score.

## Public and Private split
The public leaderboard is based on 40% of test data, while final rank would be decided on remaining 60% of test data (which is private leaderboard)

## Guidelines for Final Submission
Please ensure that your final submission includes the following:

1. Solution file containing the predicted response of the customer (Probability of response 1)
2. Code file for reproducing the submission, note that it is mandatory to submit your code for a valid final submission

# 2 | Importing the Modules
"""

# **DATASET PRE-PROCESSING**

import numpy as np 
import pandas as pd 

# **DATA VISUALIZATION**

import seaborn as sns 

# **DATA PREPROCESSING**

from sklearn.preprocessing import FunctionTransformer

# **MACHINE LEARNING MODELS**

from sklearn.neighbors import KNeighborsClassifier

# **METRICS**

from sklearn.metrics import classification_report

# **INPUT**

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

"""Lets get our data into working"""

data = pd.read_csv("/kaggle/input/health-insurance-cross-sell-prediction/train.csv")

"""It is a good practice to take a look at our dataset before processing it"""

data

"""So we have $3,81,109$ or $350K+$ rows and $12$ columns accounting to $46,52,508$ or $4.5M+$ values """

data.info()

"""There are some numerical vales and some categorical values which we need to take care 

`id` column seems to be unique for every row and thus it would be fun to remove this guy
"""

data.drop(["id"] , axis = 1 , inplace = True)

data

"""# 3 | Data Analysis and Preprocessing 

# 3.1 | Gender 

Gender can affect car insurance rates due to statistical data showing that male drivers tend to be involved in more accidents and receive more traffic violations than female drivers. Insurance companies use this data to determine risk and set premiums accordingly. However, some countries have banned the use of gender as a factor in insurance pricing.
"""

data["Gender"].value_counts().plot(kind = "pie" , autopct = "%.2f" , cmap = "gist_rainbow")

"""This is in pretty good condition"""

data["Gender"] = np.where(data["Gender"] == "Male" , 1 , 0)

"""# 3.2 | Age
Age is a significant factor that affects insurance rates for vehicles. Younger drivers, especially those under the age of $25$, typically have higher insurance rates because they are considered more risky to insure. Older drivers, on the other hand, may be eligible for lower insurance rates due to their experience and history of safe driving.
"""

sns.kdeplot(data["Age"])

"""This is highly skewed towards right. Lets try to use `FunctionTransformer` here. We will be using `np.log1p`

If you want to know how the `FunctionTransformer` works, here is the code for you , and here is the notebook containing the **[explanation](https://www.kaggle.com/code/ayushs9020/function-transformers-from-scratch-in-python)** :)

```
! pip install numpy

import numpy as np 

def log(value):
    
    return np.log1p(value)

def square(value):
    
    return np.sqaure(value)

def yeo_johnson(value , lambdas):
    
    if value >= 0:
    
        if lambdas == 0:
    
            return (np.log1p(value))
    
        else:
    
            return ((((value + 1) ** lambdas) - 1) / lambdas)
    
    if value < 0:
    
        if lambdas == 2: 
    
            return (-np.log1p(-value))
    
        else : 
    
            return ((- ((((- value) + 1) ** 2 - lambdas) - 1)) / (2 - lambdas))

def box_cox(value , lambdas):

    if lambdas == 0:

        return np.log1p(value)

    else :

        return ((((value + 1) ** lambdas) - 1) / lambdas)

def function_transformer(array , func = None):

    if func == None:
        print("Please enter a function")
        return None

    elif func == "log" or func == "square":

        output_array = np.empty(shape = array.shape)

        if len(array.shape) == 1:
            new_array = np.empty(shape = (1 , 1))
            for i in array:

                if func == "log":

                    new_array = np.vstack([new_array , log_output(i)])
                
                else:
                    
                    new_array = np.vstack([new_array , square(i)])
            
            new_array = np.delete(new_array , 0 , 0)

            return new_array

        else :
            
            for i in array:
            
                new_array = np.empty(shape = (1 , 1))
            
                for j in array[i]:
            
                    if func == "log":

                        new_array = np.vstack([new_array , log_output(i)])
                    else:
                    
                        new_array = np.vstack([new_array , square(i)])

                new_array = np.delete(new_array , 0 , 0)

                output_array = np.hstack([output_array , new_array])
                output_array = np.delete(output_array , 0 , 1)

            return output_array
    
    elif func == "box_cox" or func == "yeo_johnson":
        
        output_array = np.empty(shape = array.shape)

        if len(array.shape) == 1:

            array_list = []
            skew_list = []
            
            for constant in range(-5 , 5):
            
                new_array = np.empty(shape = (1 , 1))

                for values in array:

                    if func == "box_cox":

                        new_array = np.vstack([new_array , box_cox(values , constant)])

                    else :
                        new_array = np.vstack([new_array , yeo_johnson(values , constant)])
                
                new_array = np.delete(new_array , 0 , 0)
                
                array_list.append(new_array)
                skew_list.append(pd.DataFrame(new_array).skew()[0] ** 2)

                output_array = array_list[np.argmin(skew_list)]
            
            return output_array

        elif len(array.shape) != 1:

            for columns in array:

                array_list = []
                skew_list = []
                
                for constant in range(-5 , 5):
                    
                    new_array = np.empty(shape = (1 , 1))

                    for values in array[columns]:

                        if func == "box_cox":

                            new_array = np.vstack([new_array , box_cox(values , constant)])

                        else :
                            
                            new_array = np.vstack([new_array , yeo_johnson(values , constant)])

                    new_array = np.delete(new_array , 0 , 0)
                    
                    array_list.append(new_array)
                    skew_list.append(pd.DataFrame(new_array).skew()[0] ** 2)

                    output = array_list[np.argmin(skew_list)]

                output_array = np.hstack([output_array , output])

                output_array = np.delete(output_array , 0 , 1)

            return output_array

        else :
            print("Please enter a valid input")

    else :
        print("Please enter a valid function")
        return None
```

Though we will be using `sklearn.preprocessing.FunctionTransformer` here
"""

sns.kdeplot(FunctionTransformer(func = np.log1p).fit_transform(data["Age"]))

"""And this is really great, lets replace this with the original column"""

data = pd.concat([FunctionTransformer(func = np.log1p).fit_transform(data["Age"]) , 
                 data.drop("Age" , axis = 1)] , 
                axis = 1 , join = "inner")

data

"""# 3.3 | Driving_License
Holding a valid driving license can positively affect the cost of insurance premiums for a vehicle. Drivers with a license are considered less of a risk to insurance companies, as they have demonstrated knowledge of traffic rules and basic driving skills. On the other hand, drivers without a license or with a history of traffic violations may face higher insurance rates due to the perceived higher risk of accidents or traffic violations.
"""

data["Driving_License"].value_counts().plot(kind = "pie" , autopct = "%.2f" , cmap = "gist_rainbow")

"""This column is of no use as most of its values are same. So we will just drop it """

data.drop("Driving_License" , axis = 1 , inplace = True)

data

"""# 3.4 | Previously_Insured
If previously insured, it may indicate a responsible driving history which could result in lower insurance premiums. On the other hand, if a person has a history of accidents or claims, their previous insurance may affect their ability to obtain coverage or the cost of their premiums. Ultimately, a person's previous insurance history is one of many factors that can impact their insurance rates and eligibility for coverage.
"""

data["Previously_Insured"].value_counts().plot(kind = "pie" , autopct = "%.2f" , cmap = "gist_rainbow")

"""And this is in really good condition

# 3.5 | Vehicle Age
Vehicle age can have a significant impact on insurance premiums.
As vehicles get older, their value decreases and the cost of repairs may increase, leading to higher premiums.
Older vehicles may also be considered more of a risk to insure due to their potential for mechanical issues or safety concerns.
"""

data["Vehicle_Age"].value_counts().plot(kind = "pie" , autopct = "%.2f" , cmap = "gist_rainbow")

"""Though it has a small percentage $4.20$ of the class `>2 Years`, we will still move on with that

# 3.6 | Vehicle_Damage
Vehicle damage can significantly affect insurance premiums, especially if the driver is at fault for the accident. Insurance companies use the severity and frequency of past accidents to assess the risk of insuring a driver, and a history of vehicle damage can indicate a higher risk. In some cases, insurance coverage may also be denied or canceled due to repeated vehicle damage claims.
"""

data["Vehicle_Damage"].value_counts().plot(kind = "pie" , autopct = "%.2f" , cmap = "gist_rainbow")

data["Vehicle_Damage"] = np.where(data["Vehicle_Damage"] == "Yes" , 1 , 0)

"""# 3.7 | Annual_Premium
The annual premium of vehicle insurance can indirectly impact healthcare insurance by affecting a person's overall financial health.
If a person's vehicle insurance premium is high, they may have less disposable income to allocate towards healthcare expenses or insurance premiums.
This could lead to individuals forgoing or reducing their healthcare coverage, which could ultimately impact their health and well-being.
"""

sns.kdeplot(data["Annual_Premium"])

"""The data is higly unstable. Lets try to place a kink at $80,000$ and then see it """

data["Annual_Premium"] = np.where(data["Annual_Premium"] >80000 , 0 , data["Annual_Premium"])
sns.kdeplot(data["Annual_Premium"])

"""And now it is in a pretty good condition 
# 3.8 | Vintage 

The length of time a customer has been associated with a healthcare insurance company can have a significant impact on their overall health care coverage.
As customers continue to stay with the same healthcare insurance provider, they may receive additional benefits, such as lower premiums, better coverage, and access to more health care services.
Furthermore, long-term customers may also have a higher likelihood of receiving personalized attention and support from the insurance company, leading to better health outcomes and overall satisfaction with their coverage.
"""

sns.kdeplot(data["Vintage"])

"""Our data after a bit of preprocessing looks like this """

data

"""# 4 | Categorical Encoding

We cannot just feed `str` input to any mathematical model. We first encode them into a numeircal vector, and then we can feed them into the model. 

If you want to know how we encode values in deep, here is the code for you, and here is the **[explanation](https://www.kaggle.com/code/ayushs9020/one-hot-encoder-from-scratch)** to it

```
class OneHotEncoder:

    def __init__(self , min_frequency = None , max_categories = None , dtype = float):
        self.min_frequency = min_frequency
        self.max_categories = max_categories
        self.dtype = dtype
    def fit_transform(self , dataframe , columns):

        if type(self.min_frequency) == int:
            
            pass
        
        else : 
            
            self.min_frequency *= len(columns)

        if not self.min_frequency == None:
        
            if len(columns) == 1 :
                
                inf = [categories 
                    for categories in dataframe[columns[0]].value_counts().index 
                    if dataframe[columns[0]].value_counts()[categories] > self.min_frequency]
            
                for categories in dataframe[columns[0]].value_counts().index:
            
                    if not categories in inf:
            
                        dataframe[columns[0] + "_" + categories[0]] = np.where(dataframe[columns[0]] == categories[0] , dtype(1) , dtype(0))
            
                    else: 
            
                        dataframe[columns[0] + "_other"] = np.where(dataframe[columns[0]].isin(inf) , dtype(1) , dtype(0))
        
                    if drop == "first" or drop == "if_binary" :
        
                            dataframe.drop(str(columne[0]) + "_" + sample_data[columne[0]].value_counts.index[0] , 
                                        axis = 1 , inplace = True)
        
                    dataframe.drop(columne[0] , axis = 1 , inplace = True)
            else :
        
                for feature in columns:
                    
                    inf = [categories 
                        for categories in dataframe[feature].value_counts().index 
                        if dataframe[feature].value_counts()[categories] > self.min_frequency]
                
                    for categories in dataframe[feature].value_counts().index:
                
                        if not categories in inf:
                
                            dataframe[feature + "_" + categories] = np.where(dataframe[feature] == categories , dtype(1) , dtype(0))
                
                        else: 
                
                            dataframe[feature + "_other"] = np.where(dataframe[feature].isin(inf) , dtype(1) , dtype(0))
        
                        if drop == "first" or drop == "if_binary" :
        
                            dataframe.drop(str(feature) + "_" + sample_data[feature].value_counts.index[0] , 
                                        axis = 1 , inplace = True)
        
                    dataframe.drop(feature , axis = 1 , inplace = True) 

        elif not self.max_categories == None:
            
            if len(columns) == 1:
        
                inf = dataframe[columnes[0]].value_counts().index[self.max_categories : ]
        
                for categories in dataframe[columnes[0]].value_counts().index[: self.max_categories]:
        
                    dataframe[columnes[0] + "_" + categories[0]] = np.where(dataframe[columnes[0]] == categories[0] , dtype(1) , dtype(0))
        
                dataframe[columnes[0] + "_other"] = np.where(dataframe[columnes[0]].isin(inf) , dtype(1) , dtype(0))
        
                if drop == "first" or drop == "if_binary" :
        
                            dataframe.drop(str(columne[0]) + "_" + sample_data[columne[0]].value_counts.index[0] , 
                                        axis = 1 , inplace = True)
        
                dataframe.drop(columnes[0] , axis = 1 , inplace = True)

            else :

                for feature in columns:
            
                    inf = dataframe[feature].value_counts().index[self.max_categories : ]
            
                    for categories in dataframe[feature].value_counts().index[: max_categories]:
            
                        dataframe[feature + "_" + categories] = np.where(dataframe[feature] == categories , dtype(1) , dtype(0))
            
                    dataframe[feature + "_other"] = np.where(dataframe[feature].isin(inf) , dtype(1) , dtype(0))
        
                    if drop == "first" or drop == "if_binary" :
        
                            dataframe.drop(str(feature) + "_" + sample_data[feature].value_counts.index[0] , 
                                        axis = 1 , inplace = True)
        
                    dataframe.drop(feature , axis = 1 , inplace = True)

        else :    
            
            if len(columns) == 1:
                    
                for categories in dataframe[columns[0]].value_counts().index[0]:
                    
                    dataframe[columns[0] + "_" + categories[0]] = np.where(dataframe[columns[0]] == categories[0] , dtype(1) , dtype(0))
        
                if drop == "first" or drop == "if_binary" :
        
                            dataframe.drop(str(columns[0]) + "_" + sample_data[columns[0]].value_counts.index[0] , 
                                        axis = 1 , inplace = True)
        
                dataframe.drop(columns[0] , axis = 1 , inplace = True)

            else : 
            
                for feature in columns:
                        
                    for categories in dataframe[feature].value_counts().index[0]:
                        
                        dataframe[feature + "_" + categories] = np.where(dataframe[feature] == categories , dtype(1) , dtype(0))
        
                    if drop == "first" or drop == "if_binary" :
        
                            dataframe.drop(str(feature) + "_" + sample_data[feature].value_counts.index[0] , 
                                        axis = 1 , inplace = True)    
        
                    dataframe.drop(feature , axis = 1 , inplace = True)
```

Though here we will be using `pandas.get_dummies()`
"""

data = pd.concat([pd.get_dummies(data["Vehicle_Age"]) , 
                 data.drop("Vehicle_Age" , axis = 1)] , 
                axis = 1 , join = "inner")

data

"""# 5 | Model Building

Our data is now good to go for model building
"""

train , test = np.split(data.sample(frac = 1) , [int(0.8 * len(data))])

def pre(dataframe):
    x = dataframe.drop("Response" , axis = 1)
    y = dataframe["Response"]
    
    return x , y

X_train , y_train = pre(train)

X_test , y_test = pre(test)

"""# 5.1 | K Nearest Neighbors Classifier

KNN (K-Nearest Neighbors) is a simple and popular machine learning algorithm used for both classification and regression tasks.
In KNN, the output is predicted based on the majority class or average of the k-nearest neighbors of the input data point in the feature space.
The value of k, which represents the number of nearest neighbors, is an important hyperparameter that needs to be tuned for optimal performance.

<img src = "https://media.geeksforgeeks.org/wp-content/uploads/20200616145419/Untitled2781.png">

If you want to know more about `KNN` here is the code for you, and here is the notebook with the **[explanation](https://www.kaggle.com/code/ayushs9020/knn-from-scratch)**

```
from collections import Counter
import numpy as np 

def KNN(X_train , y_train , X_test):
    distance = [0] * X_train.shape[0]
    for columns in X_train.columns:
        for index in range(len(X_train[columns])):
            distance[index] += X_train[columns][index] - X_test[columns][index]
    distance = np.sqrt(np.array(distance))
    return Counter(y_test[np.argsort(distance)[:3]]).most_common()[0][0]
```
"""

model = KNeighborsClassifier()
model.fit(X_train , y_train)

"""# 6 | Metrics

Metrics in machine learning are used to evaluate the performance of a model.
They quantify the accuracy of a model's predictions by comparing them to the actual values.
Common metrics include accuracy, precision, recall, F1 score, and mean squared error.

We will be using the `classification report` here 
"""

print(classification_report(y_test , model.predict(X_test)))

"""And we got an accuracy of around $86$%. And thats really great. We will try to improve it in further versions

**THAT IT FOR TODAY GUYS**

**WILL BE WORKING ON IMPROVING IT FURTHER**

**PLEASE COMMENT YOUR THOUGHTS, HIHGLY APPRICIATED**

**DONT FORGET TO MAKE AN UPVOTE, IF YOU LIKED MY WORK**

**PEACE OUT !!!**
"""